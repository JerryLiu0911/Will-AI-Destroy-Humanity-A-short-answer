# Will AI Destroy Humanity?
>_"AI doesn't have to be evil to destroy humanity – if AI has a goal and humanity
just happens in the way, it will destroy humanity as a matter of course without
even thinking about it, no hard feelings." - Elon Musk_

First of all, I think its critical to point out the sense of dilemma that is being proposed here, which is “will AI’s be affected by moral values”, and therefore I will base my argument around that. Elon Musk suggests that AI’s will not be restricted by any moral values as he quotes “AI doesn’t have to be evil to destroy humanity”, implying that AI’s can perform actions which seems to go against moral values (destroying humanity in this case) without having the intention to specifically hurt people, which Musk refers to as “evil”. Before getting on to answer the question “will AI’s be affected by moral values”, let’s first take a look at what exactly is this “Artificial Intelligence”. Artificial Intelligence is created to allow robots, or algorithms to mimic human learning as of means to achieve a specific goal, such as “bake a cake”. AI’s can be then separated into different categories based on their purpose, known as : reactive machines, limited memory, theory of mind and self-awareness. For example, the famous AlphaGo is an AI that falls into the reactive machines category, as its algorithm is based on processing information of the present and deciding the most logical move (reacting to the opponents’ moves) but does not have the ability to logically plan-ahead. This is accomplished through machine learning and pattern recognition, whether through object recognition, sound recognition and other technologies to mimic the senses of humans and their logic process, Artificial Intelligence can find ways to solve a problem with given requirements for it to recognise a goal. This means the AI can be tailored to suit specific purposes and therefore fitted with abilities that allow it to gather information for solving that specific task. The interesting part is when AI’s start to solve problems in unconventional ways, or ways that the programmer didn’t intend it to. This could be seen through examples of AI’s designed to go through a maze “from one exit to the other”, skips the maze by going out of the bounds and reach the end.  This seems like a relatively small impact, since the only downside from this lack of specification in the programming only resulted in the AI not playing the maze as intended to. However, this becomes a bit more serious on the larger scale. An iconic example would be in sci-fi movies, where AI’s that were intended to “improve the Earth’s environment by reducing pollution” decided that human were the source of pollution and therefore exterminating them is the most efficient way to reduce pollution. This would be what Musk was referring to in his quote, “if AI has a goal and humanity just happens in the way, it will destroy humanity as a matter of course”. And to argue if there was a dilemma for the AI, then the answer would be a straightforward “no”, as to them they were only instructed to achieve the goal of “reducing pollution”, and therefore the AI sees “removing humans” as a viable option just as “recycling used objects” for example. This then brings to the question “well can’t we just put moral restrictions on them like humans do?”. Just as we’ve mentioned before, even if we do add restrictions onto the algorithm it is highly possible that the AI will find other interpretations of these restrictions and proceed to complete the task in other unconventional ways of exploitation. Since laying down rules won’t work, wouldn’t it be easier to let the AI identify them themselves? A big part of the reason why humans are able to be restricted by moral values is because we have the ability to feel pain and sympathy for others, to recognise unfairness and inequality, and more importantly a consciousness to not harm others. To introduce these to AI then brings along more questions that may go much further down in the rabbit hole of  “What is consciousness” and “If AI’s have consciousness, is it still moral to use them as tools?” etc. Before these questions are answered, we will be unable to put on moral restrictions, or even be ill-equipped when AI’s do gain moral value. Needless to say, without restrictions AI can be just as a convenience as they can be dangerous if we implement them into greater roles. Will AI’s be the dusk of humanity? Not yet. However they undoubtedly can be.
